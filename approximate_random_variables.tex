\documentclass[11pt,a4paper,oneside,english]{extarticle}
\input{preamble}
\makeatletter
\fancyhead[LO]{\@title}
\makeatother

\newcommand{\singlecodeline}[1]{\\[1em]\centerline{\lstinline[basicstyle=\ttfamily]$#1$}\\[1em]}


\title{Approximate random variables: A brief guide for practitioners}
\author{\href{mailto:oliver.sheridan-methven@maths.ox.ac.uk}{Oliver Sheridan-Methven}%
\thanks{\href{mailto:oliver.sheridan-methven@maths.ox.ac.uk}%
{\texttt{oliver.sheridan-methven@maths.ox.ac.uk}}}
}
\date{\datedayname\ \nth{\number\day} \monthname\  \number\year}

\begin{document}
\linenumbers
\maketitle
\pagenumbering{arabic}

\begin{abstract}
We introduce the idea of \emph{approximate random variables} and discuss when they are useful. We provide code bundles in Python and C which demonstrate various implementations and use-cases, showing key results, and how to obtain these yourself using the code provided.  
\end{abstract}

\tableofcontents

\section{Introduction}

This report is intended to be a relatively light introduction to approximate random variables, designed primarily for an industry practitioner, but also applicable to scientists who want to achieve better code performance. In this introduction, we will give a gentle introduction to random numbers and approximate random variables, and discuss one of their major areas of application. We will give a light discussion of what's going on from a mathematical perspective, and provide code demonstrating how to implement these ideas. Most of the code be written using Python in an easy to follow manner (not particularly focused on performance), and where appropriate we will also provide some high performance code written in C. We encourage readers to use either the code provided as is, or adapt it to their own needs as appropriate. The code largely demonstrative, so we make no guarantees about its performance or correctness. 

\subsection{Where can I learn more?}
\label{sec:where_can_i_learn_more}

For people who are more comfortable with maths and want to know more, there are several other sources of reference material available, of various degrees of technicality. From easiest to hardest there is:
\begin{itemize}
\item \emph{High-performance low-precision vectorised arithmetic and its applications}, Oliver Sheridan-Methven, 2020, 2~pages. This is an extremely brief overview of my research, giving a light overview of the core idea and some key findings, aimed at a very general reader with little technical knowledge. 

\item \emph{High-performance low-precision vectorised arithmetic and its applications}, Oliver Sheridan-Methven, 2020, 8~pages. This is a very brief overview of my research, going into some more detail than the 2~page version. This is aimed at those with a bit more technical knowledge. 

\item \emph{Analysis of nested multilevel Monte Carlo using approximate Normal random variables}, Mike Giles and Oliver Sheridan-Methven, 2020,  approximately 25 pages. This is a journal article currently in preparation and due to be released very soon. This is aimed at a technical reader, and goes into depth with the main technical idea of our research and how to apply it, and is suitable for readers with a science degree.

\item \emph{Nested multilevel Monte Carlo methods and a modified Euler-Maruyama scheme utilising approximate Gaussian random variables suitable for vectorised hardware and low-precisions}, Oliver Sheridan-Methven, 2020, 250~pages. This is my PhD thesis, and goes into all the technical mathematical details surrounding these ideas. This is not a particularly light, easy, nor entertaining read, but is likely better suited as a manual for anyone worried about the technical consequences of switching to approximate random variables. 
\end{itemize}


\subsection{What are random variables?}

Random numbers are part of everyday life, whether rolling dice, reading stock prices, or even sending encrypted messages, random numbers are a part of all of these. As random numbers occur so frequently in the world around us, they also play a central role in various branches of science and mathematics. Usually, when some random quantity appears is part of a scientific equation or mathematical formula, its given the more formal technical name of a being a \emph{random variable}, which is the name we'll use going forward. Studying how random variables behave is central to many branches of science, and has applications in gambling, finance, computer simulations, weather prediction, security and encryption, artificial intelligence, etc., the list goes on and on.  

Computers often need random numbers, either for running simulations or encrypting data. However, computers don't behave randomly, and follow a very strict set of rules and instructions. So for a computer to spit out a sequence random numbers, it follows a very long and complicated set of steps, such that the numbers it spits out, while technically not random, appear random enough for most purposes. In fact, producing high quality random numbers can be a time consuming task for a computer.

The problem is made even harder when we consider that there are different varieties of random variables. In science, we say a given variety of random variables follows a \emph{distribution}, which encompasses a particular style of random numbers. Some of these you are likely familiar with and already have names, where some common distributions include:

\begin{itemize}
\item The Bernoulli distribution (e.g.\ a coin toss).
\item The discrete uniform distribution (e.g.\ the roll of a die).
\item The continuous uniform distribution (e.g.\ an individual's height percentile).
\item The Poisson distribution (e.g.\ lifetime of a lightbulb). 
\item The Gaussian distribution (e.g.\ average height of adults).
\end{itemize}

Computers by default are usually only able to produce the uniform distributions, and there are various methods to get other more complicated distributions from these simpler ones. Amongst these methods, one which is particularly robust and versatile is known as \emph{the inverse transform method}.

\subsubsection{The inverse transform method}

For some distribution of interest, let's suppose we know where the percentiles of the distribution lie, e.g. 50\% of the values are below 0, 70\% are below 1, 90\% are below 2, 99\% are below 3, 99.99\% are below  4, etc. We can easily sample a uniform distribution, but how can we sample from this more interesting distribution? The way the inverse transform method works is surprisingly simple. Sample a uniform random variable, giving a number in 0--100, treat this as a percentile, and see what value of the distribution this percentile corresponds to. The number corresponding to this particular percentile is a random variable which exactly follows the desired distribution. There are only two things you need for this: access to uniform random numbers (which almost all computers have), and a formula for the percentiles. Repeating this for several uniformly distributed random variables results produces random variables from the desired distribution, where a simple demonstration is shown in \Cref{fig:inverse_cdf_transformation_method_schematic_simplified}.

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\linewidth, clip=true, trim={5mm, 2mm, 5mm, 5mm}]{inverse_cdf_transformation_method_schematic_simplified}
\caption[The inverse transform method]{The inverse transform method produces random variables following a desired pattern.}
\label{fig:inverse_cdf_transformation_method_schematic_simplified}
\end{figure}

The technical name for the expression which takes a percentile and determines what value of a random variable this corresponds to is \emph{the inverse cumulative distribution function}, sometimes called the inverse CDF or the percentage point function. Most computers and programming languages have access to these functions for some of the most popular distributions. 

\subsection{What are approximate random variables?}
\label{sec:what_are_approximate_random_variables}

So far we have painted a trouble free picture for producing random variables from whatever distribution interests by using the inverse transform method. But suppose we have an application, such as a computer simulation, which requires lots of random variables from a distribution. As an example, it is not uncommon for financial simulations of stock markets to use anywhere from millions to trillions of random numbers to predict the value of various financial products. If this is the case, then we want to process these random numbers as quick as possible. Unfortunately, while the inverse transform method is versatile, if you do it exactly, it can be very slow and struggle to perform well on the latest hardware. This is where our research proposes the following idea: don't do it exactly, do it approximately. 

To see where we make our approximations, lets take a very important and ubiquitous distribution known as the Gaussian distribution, which is sometimes called the Normal distribution or the bell curve. The Gaussian's inverse cumulative distribution function is plotted in \Cref{fig:inverse_cdf_function_tails_simplified},
where we indicating several random positions the function might evaluate. Most of the inputs land near the centre of the function, where it's nearly a straight line and easy to evaluate. However, sometimes values land near the edges, in the more difficult shaded region where the function shoots off the edges of \Cref{fig:inverse_cdf_function_tails_simplified}, and is much harder to evaluate. While these difficult scenarios are infrequent, the greater the degree of parallelisation, the more certain it is that such values are unavoidable. It is these types of edge values which contaminate an otherwise easy calculation, leading to a loss in performance. 

\begin{figure}[htb]
\centering
\begin{subfigure}[t]{0.45\linewidth}
\centering
\includegraphics[width=\linewidth]{inverse_cdf_function_tails_simplified}
\begin{minipage}[t]{0.9\linewidth}
\caption{The inverse Gaussian cumulative distribution function, and several possible random inputs (\CIRCLE). The more difficult regions are shaded, where some random inputs will unfortunately land ({\color{red}\CIRCLE})}
\label{fig:inverse_cdf_function_tails_simplified}
\end{minipage}
\end{subfigure}%
~
\begin{subfigure}[t]{0.45\linewidth}
\centering
\includegraphics[width=\linewidth]{inverse_cdf_uniform_discretisation_unshaded_simplified}
\begin{minipage}[t]{0.9\linewidth}
\caption{An example approximation of the exact function. The approximation is several flat lines of equal widths.}
\label{fig:inverse_cdf_uniform_discretisation_unshaded_simplified}
\end{minipage}
\end{subfigure}
\caption{The Gaussian's inverse cumulative distribution function and an approximation.}
\label{fig:gaussian_inverse_cdf}
\end{figure}


To remedy the difficult edge cases, our research proposes introducing an approximation to the exact inverse cumulative distribution function. The approximation is designed to tackle the edge cases as easily as the central ones, and being fast irrespective of what's encountered. One example we proposed and studied was approximating the function by a series of flat lines, where an example is shown in \Cref{fig:inverse_cdf_uniform_discretisation_unshaded_simplified}. 

In our experiments, we pitted various exact implementations (some proprietary), against our approximations. Although inexact, our approximations produced random variables in a fraction of the time it takes the exact functions. The random variables produced don't exactly follow the desired distribution, but instead something very similar. When the random variables are produced  using an approximation, we call them \emph{approximate random variables}. In our research, we study the error introduced from using these instead of the exact ones. Not only can we quantify how they affect the answer, we developed a mathematical approach to counteract their error. This means we can reap the benefits of the faster speed, without losing any accuracy, getting the best of both worlds. 

\subsection{When would I use approximate random variables?}

In our research we investigated and analysed the consequences of using approximate random variables in the context of running computer simulations. This is extremely common in finance, and also used in weather forecasting. As we mentioned, various other applications use random numbers and could likely benefit from using approximate random numbers, although there remains work to be done to investigate and analyse all of the remaining use-cases.

One particularly prolific scientific method is known as \emph{Monte Carlo}, and it heavily used for financial simulations, and requires vast quantities of random numbers, usually from the Gaussian distribution. The Monte Carlo method can be described as: run lots of simulations of whatever is of interest, compute the average of what you see, and that's approximately the answer. The more simulations you run the more accurate your answer is. For most people, computing averages is easy, but how does someone ``simulate''? Running a simulation is asking a computer to do the following: given where something is now, and knowing how it behaves, guess where it will be in the future? When we say we know how the system behaves, it means we have an expression which accurately describes where it will evolve. This expression though is only accurate at predicting a tiny bit into the future. To predict far into the future, one method is to split this up into lots of smaller intermediate predictions. We then predict from one step to the next, one after another, until you're at the end. 


The simplest method of simulating something step-by-step is known as the \emph{Euler scheme}, and if the evolution is subject to random influences it's the \emph{Euler-Maruyama scheme}. There are two factors influencing the evolution: one isn't random, and the other is (e.g.~traders buying and selling stocks randomly influences their price). To simulate the random bit we need a random number, and the Euler-Maruyama scheme needs them from a Gaussian distribution. Some example Euler-Maruyama simulations are shown in \Cref{fig:example_simulation}.

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\linewidth]{example_simulation}
\caption{A non-random process (\rule[0.2em]{1.5em}{2pt}), and three Euler-Maruyama simulations ({\color{red}\rule[0.2em]{1.5em}{2pt}}).}
\label{fig:example_simulation}
\end{figure}

The Euler-Maruyama scheme is simple, a robust problem solver, and there's not much that it struggles with, so why tamper with it? One shortcoming is that at each step it requires a random number from the Gaussian distribution. Unfortunately, this is usually the most expensive bit. To give the scheme the random numbers it needs quicker, we propose swapping them for our approximate random numbers. 


Modifying the Euler-Maruyama scheme to substitute exact random numbers with approximate ones largely alleviates the bottleneck.  We can now run simulations much faster, improving the accuracy of any Monte Carlo estimate which uses the Euler-Maruyama scheme to produce the simulations. However, we cannot overlook that we compromised the quality of our simulations, and this will introducing some error into our answer. Is this significant though, or even noticeable? In our research, we've shown that the error introduced is limited in size by the error arising from approximating the Gaussian distribution. So to reduce the resulting error to an acceptable level, it's sufficient to just increase the fidelity of our approximation.

\subsection{How can I get started using them?}

All you need to get started using approximate random variables is an approximation and a problem to solve. In the code provided we will give some examples of approximating the Gaussian distribution, where we've taken some time and effort to ensure our approximations are fast. We also deploy these on some example problems to give a demonstration of how to use these ideas. Before using approximate random variables in your own setting, we encourage you go through some of the demonstrations provided to give a better idea of how to incorporate and adapt them as necessary into your own problem. We also show how one of our approximations to the Gaussian distribution can be adapted to approximate another distribution, and we expect our approximations can be used as templates for approximating several other distributions. 

Approximate random variables are as their name suggests: approximate. Switching to approximate random variables is easy, and can gain you huge amounts of speed. Unfortunately, swapping the exact random variables for the approximate ones and doing nothing else, while easy to code up, will introduce some error. However, with a little extra effort, by utilising a technique known as \emph{multilevel Monte Carlo}, you can counteract this error without compromising speed, and obtain the best of both worlds. This can be a little technical, and we refer a reader interested in the technicalities of the method to some of the additional material mentioned in \Cref{sec:where_can_i_learn_more}.


\subsection{How do I run these demonstrations?}

The demonstrations we provide will mostly be written in Python, and a few higher performance versions will be in C. For the Python code, we will be using Python~2, although it should all be easily portable to Python~3 with minimal modification. The Python files can either be run from a Python IDLE, or from the command line, whichever is easier. We assume a Unix or GNU/Linux based operating system. The C files will come with their own makefiles, and will usually assume access to the \texttt{gcc} compiler. Some of the C programs are not hardware agnostic, and will require altering the makefiles in places. For some applications, we may be comparing performance against \intel or \nag libraries, and assessing performance on \intel, \arm, or even \nvidia hardware. Thus will assume the user has access to:
\begin{itemize}
\item The GNU scientific library (GLS) and the \texttt{gcc} compiler.
\item The \intel maths kernel library (MKL), the vector statistics library (VSL), and the \texttt{icc} compiler. 
\item The \nag library. 
\item The \arm \texttt{armclang} compiler.
\item The \nvidia \texttt{nvcc} compiler.
\end{itemize} 
Similarly, we will assume access to appropriate hardware from either \intel, \arm, or \nvidia where appropriate. We assume when compiling that the user has also set up the default paths to find the header files and libraries by appropriately setting the environment variables \verb|LIBRARY_PATH| and \verb|C_INCLUDE_PATH|.

We will try an keep the python packages used to a minimum, although some of the notable packages which are used within our code include: \texttt{numpy}, \texttt{pandas}, \texttt{scipy}, \texttt{mpmath}, \texttt{matplotlib}, etc. 

\section{Piecewise constant approximations}

We mentioned earlier in \Cref{sec:what_are_approximate_random_variables} an approximation to the Gaussian's inverse cumulative distribution function which was comprised of a series of flat lines of equal width. The mathematical name for this approximation is a \emph{piecewise constant function} which uses \emph{equipartitioned intervals}, although we will just abbreviate this as the piecewise constant function. To construct the approximation, we first have to decide, how many intervals do we need, and what value should be use as the constant for each interval. 

The number of intervals is the easy bit, so let's say we have $ N $ of them, where in most of our approximations we found $ N = 1028 $ to be sufficiently high to obtain a reasonable fidelity approximation. As for what value to use, a sensible choice is whatever you would expect to get from the exact function. The exact inverse cumulative distribution function for the Gaussian distribution is usually denoted $ \Phi^{-1} $. Denoting the start and end of a particular interval as $ a $ and $ b $ respectively, where $ a < b $, the value used for a particular interval is
\begin{equation}
\dfrac{1}{b - a}\int_a^b \Phi^{-1}(x) \dd{x} \equiv \dfrac{1}{b - a} \int_{\Phi^{-1}(a)}^{\Phi^{-1}(b)} \dfrac{z}{\sqrt{2\pi}} \exp(-\dfrac{z^2}{2}) \dd{z}.
\end{equation}
Both these integrals are equivalent to one another, where $ \Phi $ is the Gaussian's cumulative distribution function (i.e.~not the \emph{inverse} CDF). Both are easy for a computer to calculate, where the second is sometimes more convenient for intervals touching the edge. 

If we label the intervals $ 0, 1, 2,\ldots, N-1 $, we can store these values in an list. For a uniform random input $ u $ in the range $ [0, 1) $, where 1 is not included, we can obtain the desired value by simply looking the index corresponding to the value. Mapping $ u $ to the index is done by multiplying by $ N $ and only keeping the integer part, and hence computes $ \lfloor u N \rfloor $, which in code can be done my something equivalent to \verb|int(u * N)|.

In the file \singlecodeline{approximate_gaussian_distribution.py} there is a function \singlecodeline{construct_piecewise_constant_approximation} which produces a piecewise constant approximation to the Gaussian's inverse cumulative distribution function. A condensed version is shown in \Cref{code:python:construct_piecewise_constant_approximation}.

\begin{lstfloat}[htb]
\begin{lstlisting}[language=python, captionpos=b, caption={Constructing a piecewise constant approximation.}, label={code:python:construct_piecewise_constant_approximation}]
def expected_value_in_interval(func, a, b):
    """ Calculates the expected value of a function inside an interval. """
    return integrate(func, a, b) / (b - a)


def build_lookup_table(func, n_table_entries):
    """ Builds a lookup table. """
    interval_width = 1.0 / n_table_entries
    lookup_table = zeros(n_table_entries)
    for n in progressbar(range(n_table_entries)):
        a = n * interval_width
        b = a + interval_width
        lookup_table[n] = expected_value_in_interval(func, a, b)
    return lookup_table


def construct_piecewise_constant_approximation(func, n_intervals):
    """ Constructs a piecewise constant approximation. """
    lookup_table = build_lookup_table(func, n_intervals)

    def piecewise_constant_approximation(u):
        """ A piecewise constant approximation. """
        return lookup_table[array(n_intervals * u).astype(int)]

    return piecewise_constant_approximation
\end{lstlisting}
\end{lstfloat}

In the file \singlecodeline{approximate_gaussian_distribution_demonstrations.py}
there is the function  
\singlecodeline{plot_piecewise_constant_approximation_of_gaussian}
which demonstrates how to use this approximation, and how you can reproduce \Cref{fig:inverse_cdf_uniform_discretisation_unshaded_simplified} for yourself. A condensed version of the function is shown in \Cref{code:python:plot_piecewise_constant_approximation_of_gaussian}, where we exclude the values 0 and 1 as the exact function struggles with these values (returning infinite values). The output from \Cref{code:python:plot_piecewise_constant_approximation_of_gaussian} is shown in \Cref{fig:piecewise_constant_approximation_of_gaussian}.

\begin{lstfloat}[htb]
\begin{lstlisting}[language=python, captionpos=b, caption={Comparing the exact and approximate functions.}, label={code:python:plot_piecewise_constant_approximation_of_gaussian}]
uniform_input = linspace(0, 1, 1000)[1:-1]  # We exclude the end points.
approximate_inverse_gaussian_cdf = \
    construct_piecewise_constant_approximation(
        inverse_gaussian_cdf, 
        n_intervals=8)
plot(uniform_input, inverse_gaussian_cdf(uniform_input))
plot(uniform_input, approximate_inverse_gaussian_cdf(uniform_input))
\end{lstlisting}
\end{lstfloat}

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\linewidth]{piecewise_constant_approximation_of_gaussian}
\caption{The piecewise constant approximation produced by \Cref{code:python:plot_piecewise_constant_approximation_of_gaussian}.}
\label{fig:piecewise_constant_approximation_of_gaussian}
\end{figure}

Although the Python implementation is not optimised to be very high speed (compared to our C version), it is still much faster than the exact routine (\verb|norm.ppf| from \verb|scipy.stats|). On my machine, the function \singlecodeline{piecewise_constant_approximation_of_gaussian_timing} from the file \singlecodeline{approximate_gaussian_distribution_demonstrations.py} gives the following estimates for the average time:
\begin{verbatim}
Average time for the approximate function: 3.23281e-08 s.
Average time for the exact function: 8.69456e-07 s.
\end{verbatim}
from which we can see the approximation is approximately 30 times faster than the exact function. Of course, both of these times are much slower than equivalent C code, but it is at least indicative that there is a lot of speed to be gained. 


It is worth noting of course that the lookup table only needs to be computed once, and this can be done offline, so is not performance critical. 

So how can a higher performance version be implemented in C? Easily. Assuming we've precomputed the values for the lookup table, then these can be stored in an array, and the piecewise constant approximation is a effectively a single line of C code. A slightly expanded version of the function \singlecodeline{piecewise_constant_approximation}
from the file \singlecodeline{piecewise_constant_approximation.c}
is shown in \Cref{code:c:piecewise_constant_approximation}. We can see the implementation condenses down to a single simple line of code. The reason this is so fast is because the lookup table is small enough that it will easily fit within the L2-cache. Furthermore the lookup is unconditional, and so well suited to vectorisation, and hence we explicitly mark the process as suitable for vectorisation using \verb|#pragma omp simd|.

\begin{lstfloat}[htb]
\begin{lstlisting}[style=C, captionpos=b, caption={Constructing a piecewise constant approximation.}, label={code:c:piecewise_constant_approximation}]
#include <omp.h>

#define LOOKUP_TABLE_SIZE 1024
const double lookup_table[LOOKUP_TABLE_SIZE] = {-3.37,-2.98,...,2.98,3.37};

void piecewise_constant_approximation(
    unsigned int n_samples, 
    const double *restrict input, 
    double *restrict output)
{
    #pragma omp simd
    for (unsigned int n = 0; n < n_samples; n++)
    {
        output[n] = lookup_table[(unsigned int) (LOOKUP_TABLE_SIZE * input[n])];
    }
}
\end{lstlisting}
\end{lstfloat}

To compare this against the equivalent GSL function (\verb|gsl_cdf_ugaussian_Pinv|), there is a simple series of compiler instructions in the file \singlecodeline{make_time_gaussian_approximations.sh} which resembles 
\begin{verbatim}
gcc -I. -c piecewise_constant_approximation.c
gcc -I. -O0 -c time_gaussian_approximations.c
gcc -I. -o time_approximations *.o -lgsl
\end{verbatim}
and produces the executable \verb|time_approximations|. On my machine, which is not vector capable, and running with minimal optimisations we obtain
\begin{verbatim}
Average time for the approximate function: 2.28712e-09 s.
Average time for the exact (GSL) function: 2.18994e-08 s.
\end{verbatim}
Similar to before the approximation is 10 times faster than the exact function from GSL, without any optimisation. Furthermore, both functions are running 10 times faster in C than in Python, which is not too surprising. If we compile \singlecodeline{piecewise_constant_approximation.c}
under \verb|-O3| then this changes to 
\begin{verbatim}
Average time for the approximate function: 9.3289e-10 s.
Average time for the exact (GSL) function: 2.18471e-08 s.
\end{verbatim}
widening the speed increase to nearly 25 times. With vector capable hardware and some fine tuning with OpenMP, the method can be very competitive. 


\section{Piecewise polynomial approximations}


There are a few natural ways to extend the piecewise constant approximation, and the two most obvious are: increase the order of the polynomial, and change the spacing of the intervals so they're denser around the edges. 

Starting with increasing the order of the polynomial, the next improvement from a piecewise constant is a piecewise linear function. So the next question is which is the linear function best approximates the function in a given interval. There are various measures of the error, including the worse case error, or the average case error. For Monte Carlo applications, we used a variant called \emph{the mean squared error}, known mathematically as \emph{the $ L^2 $-error}. One of the advantages of the $ L^2 $-error, is that it is relatively easy to determine the optimal parameters. For a function $ f $, let's approximate this using a function $ \tilde{f} $, where we write it as an $ m $-th order polynomial  
\begin{equation}
f(u) \approx \tilde{f}(u) = c_0 + c_1 u + c_2 u^2 + \cdots + c_m u^m. 
\end{equation}
So how can we determine the optimal coefficients? The error can be expressed as 
\begin{equation}
\int_{a}^{b} \parens{f(u) - \sum_{i=0}^{m}c_i u^i}^2 \dd{u}.
\end{equation}
We can minimise this by differentiating with respect to each coefficient $ c_i $ and finding the values which make the derivative zero. Doing this for each of the coefficients, we obtain the set of $ m + 1 $ simultaneous equations 
\begin{equation}
\sum_{i=0}^{m} c_i \parens{\dfrac{b^{i+j+1} - a^{i + j + 1}}{i + j + 1}} = \int_{a}^{b} u^j f(u) \dd{u}
\end{equation}
for $ j = 0,1,2,\ldots,m $, which is equivalent to an equation of the form 
\begin{equation}
A\bm{x} = \bm{b}.
\end{equation} 
Luckily, solving simultaneous equations of this form is easy for computers, and so the coefficients are easily determined, where 
\begin{gather}
\bm{b}_i = \int_{a}^{b} u^i f(u) \dd{u} \\
\shortintertext{and}
A_{i,j} = \dfrac{b^{i+j+1} - a^{i + j + 1}}{i + j + 1}, \\
\shortintertext{and we are solving for values of $ \bm{x} $, where}
\bm{x}_i = c_i.
\end{gather}

The improvement we also want to make is to make the intervals denser near the edges of the function, as these are the trickiest bits requiring the most effort from an approximation. One way to achieve this is to have the intervals geometrically decaying in size near the edge, and one decay rate which is particularly well suited for computer calculations is in powers of two (as computers work in binary). When the intervals are decaying powers of two, we give these the special name of \emph{dyadic intervals}.

All inverse cumulative distribution functions are defined for valued between 0 and 1, and so we only need dyadic intervals in this range. If we again consider $ N $ intervals in this range, each corresponding to an index, they are as shown in \Cref{tab:dyadic_intervals_array_indices}.

\begin{table}[htb]
\centering
\begin{tabular}{cc}
Dyadic interval         & Index \\ \hline
$ [\tfrac{1}{2}, 1) $           &          0           \\
$ [\tfrac{1}{4}, \tfrac{1}{2}) $     &          1           \\
$ [\tfrac{1}{8}, \tfrac{1}{4}) $     &          2           \\
$ \vdots $                &      $ \vdots $      \\
$ [\tfrac{1}{2^{n+1}}, \tfrac{1}{2^n}) $ &        $ n $ \\
$ \vdots $                &      $ \vdots $      \\
$ [0, \tfrac{1}{2^{N-1}}) $ &        $ N-1 $
\end{tabular}
\caption{The indices for various dyadic intervals.}
\label{tab:dyadic_intervals_array_indices}
\end{table}

These intervals are dense as near zero, but they are not dense near 1. Is this a problem? Not really. For the Gaussian's inverse cumulative distribution function, this is rotationally symmetric about $ \tfrac{1}{2} $ such that $ \Phi^{-1}(u) \equiv -\Phi^{-1}(1-u) $, and so values near one are easily computed by the equivalent value reflected about $ \tfrac{1}{2} $. Notice that if we reflect about $ \tfrac{1}{2} $, then there will be one interval (the 0 interval), which will contain just the value at $ \tfrac{1}{2} $. While this a slight computational redundancy, ensuring this value has an interval in which it belongs ensures any approximation won't break if it is given the perfectly valid value of $ \tfrac{1}{2} $.

Taking the an order-1 polynomial, corresponding to a piecewise linear function, in the file \singlecodeline{approximate_gaussian_distribution.py} the function \singlecodeline{construct_symmetric_piecewise_polynomial_approximation} provides a relatively simple demonstration of how to build and implement such a function. A small demonstration of how to use this can be found in function \singlecodeline{plot_piecewise_polynomial_approximation_of_gaussian} from \singlecodeline{approximate_gaussian_distribution_demonstrations.py}
where the output is shown in \Cref{fig:piecewise_polynomial_approximation_of_gaussian}. If the number of intervals in increased to a modest amount (e.g.~8 or 16) then the approximation achieves a very high fidelity very quickly. 

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\linewidth]{piecewise_polynomial_approximation_of_gaussian}
\caption{A piecewise linear approximation using dyadic intervals.}
\label{fig:piecewise_polynomial_approximation_of_gaussian}
\end{figure}

It is worth noting that the Python implementation provided in 
\singlecodeline{approximate_gaussian_distribution_demonstrations.py}
is only demonstrative, but is not particularly high performance, where the function \singlecodeline{piecewise_constant_polynomial_of_gaussian_timing}
produces
\begin{verbatim}
Average time for the exact function: 8.93741e-08 s.
Average time for the approximate function: 1.22783e-06 s.
\end{verbatim}
showing that our Python implementation is in approximately 15 times slower than the exact function! 

To get an idea of how accurate these approximations are for different numbers of intervals and varying polynomial orders, the function \singlecodeline{plot_error_of_piecewise_polynomial_approximation_of_gaussian} computes the RMSE for various configurations, with the results shown in \Cref{fig:piecewise_polynomial_approximation_of_gaussian_rmse}.

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\linewidth]{piecewise_polynomial_approximation_of_gaussian_rmse}
\caption{Error of piecewise polynomial approximations.}
\label{fig:piecewise_polynomial_approximation_of_gaussian_rmse}
\end{figure}

We are not particularly discouraged by this Python result, as this approximation can be extremely performance on vector hardware, assuming a sufficiently wide vector length. The reason for this is simple, let us suppose we are working in 32-bit single-precision, and we are using the reasonably high fidelity 16-interval piecewise cubic approximation. If we store the possible coefficients for a given term in a list, then this will only require 512-bits ($ 32 \times 16 = 512 $), which is the vector width on a lot of modern hardware, such as \intel's AVX512 Skylake hardware or newer. This means that when we lookup the coefficients for a given term, instead of querying the cache, we can query a vector register, where we will only need 4 vector registers to hold all the coefficients we need. Additionally, if we are using dyadic intervals, then we can easily determine the index by reading the exponent bits of the floating-point number using a bit-wise integer operations (much quicker than how we did this in Python). This all means we never even need to look in the cache, and everything we require is held in the cache, and we only need a few simple arithmetic operations (which can use FMAs) and bit manipulations. This means that a C implementation can achieve very high speeds. 

\section{The Milstein scheme}

\section{The non-central $ \chi^2 $-distribution}

\section{Comparing against exact library implementations}

\end{document}
